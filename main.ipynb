{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib64:/usr/local/lib:/usr/lib64\r\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 21 01:20:22 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GT 610      Off  | 0000:01:00.0     N/A |                  N/A |\r\n",
      "| 40%   45C    P8    N/A /  N/A |      0MiB /   963MiB |     N/A      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN X (Pascal)    Off  | 0000:02:00.0     Off |                  N/A |\r\n",
      "| 33%   50C    P8    19W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN X (Pascal)    Off  | 0000:03:00.0     Off |                  N/A |\r\n",
      "| 27%   47C    P8    16W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  TITAN X (Pascal)    Off  | 0000:83:00.0     Off |                  N/A |\r\n",
      "| 27%   47C    P8    19W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  TITAN X (Pascal)    Off  | 0000:84:00.0     Off |                  N/A |\r\n",
      "| 27%   48C    P8    18W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0                  Not Supported                                         |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "if cuda_available:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "context_size = 4\n",
    "embed_size = 300\n",
    "\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "batch_size = 100\n",
    "l_rate = 0.001\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n"
     ]
    }
   ],
   "source": [
    "categories = ['comp.sys.ibm.pc.hardware','comp.sys.mac.hardware']\n",
    "raw_data = fetch_20newsgroups(subset = \"train\", categories = categories)\n",
    "print(list(raw_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_num 1168\n"
     ]
    }
   ],
   "source": [
    "data_num = len(raw_data.data)\n",
    "print(\"data_num\",data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary and word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From randy lynx msc cornell edu UUCP ( Randall Jay Ellingson , 199 Clark , 55915 , ) Subject Re IDE vs SCSI Originator randy msc2 msc cornell edu Organization Cornell Materials Science Center Lines 47 From article 1qq7i1INNdqc dns1 NMSU Edu , by bgrubb dante nmsu edu ( GRUBB ) wlsmith valve heart rri uwo ca ( Wayne Smith ) write In article 1qpu0uINNbt1 dns1 NMSU Edu bgrubb dante nmsu edu ( GRUBB ) writes wlsmith valve heart rri uwo ca ( Wayne Smith ) writes Since the Mac uses ONLY SCSI 1 for hard drives YES the figure includes a hundred for SCSI drivers This is sloppy people and DUMB What group is this ? This is not a MAC group Nice of you to DELETE BOTH YOUR responce and the item that prompted it to whit I just bought at Quantum 240 for my mac at home I paid 369 for it I Tons of stuff deleted on SCSI vs IDE question Wow , you guys are really going wild on this IDE vs SCSI thing , and I think it 's great ! Like lots of people , I 'd really like to increase my data transfer rate from the hard drive Right now I have a 15ms 210Mb IDE drive ( Seagate 1239A ) , and what I would say is a standard ( not special , no cache I believe ) IDE controller card on my ISA 486 50 I'm currently thinking about adding another HD , in the 300Mb to 500Mb range And I'm thinking hard ( you should hear those gears a grinding in my head ) about buying a SCSI drive ( SCSI for the future benefit ) I believe I'm getting something like 890Kb sec transfer right now ( according to NU ) How would this number compare if I bought the state of the art SCSI card for my ISA PC , and the state of the art SCSI hard drive ( the wailing est system I could hope for ) ? Obviously money factors into this choice as well as any other , but what would YOU want to use on your ISA system ? And how much would it cost ? Along those lines , what kind of transfer rate could I see with my IDE HD 's if I were to buy the top of the line IDE caching controller for my 200Mb , 15ms HD ? And how much would it cost ? I actually have a PAS 16 , and could ( what a waste I guess it would be ) hook up a SCSI HD through it 's SCSI port which yields an optimum of 690Kb sec Actually , I have a borrowed 12ms Fujitsu HD hooked up through it now ( and own the Trantor HD drivers for the PAS 16 SCSI port ) Is this SCSI port a SCSI 2 port ? How could I tell ? Is the Fujitsu 2623A a SCSI 2 ? Are all SCSI HD 's SCSI 2 ? Thanks for any comments on these rephrased questions Randy\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    # Original : https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    # Tips for handling string in python : http://agiantmind.tistory.com/31\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "clean_str(raw_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'randy', 'lynx', 'msc', 'cornell']\n"
     ]
    }
   ],
   "source": [
    "# Below line is needed to use nltk tokenizer\n",
    "# https://stackoverflow.com/questions/37101114/what-to-download-in-order-to-make-nltk-tokenize-word-tokenize-work\n",
    "# nltk.download(\"punkt\")\n",
    "print(word_tokenize(clean_str(raw_data.data[0]))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list_size 272692\n",
      "vocab_size 21536\n"
     ]
    }
   ],
   "source": [
    "# This step might take time\n",
    "word_list = [word for data in raw_data.data for word in word_tokenize(clean_str(data))]\n",
    "vocab = np.unique(word_list)\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "word_list_size = len(word_list)\n",
    "vocab_size = len(vocab)\n",
    "print(\"word_list_size\", word_list_size)\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct co-occurence matrix : This won't be generated if vocab_size is too big due to memory problem.\n",
    "co_occurence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "for i in range(word_list_size):\n",
    "    for j in range(1, context_size + 1):\n",
    "        index = word_to_index[word_list[i]]\n",
    "        if i-j > 0:\n",
    "            left_index = word_to_index[word_list[i-j]]\n",
    "            co_occurence_matrix[index, left_index] += 1.0/j\n",
    "        if i+j < word_list_size:\n",
    "            right_index = word_to_index[word_list[i+j]]\n",
    "            co_occurence_matrix[index, right_index] += 1.0/j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' co_oc Matrix is shifted in order to prevent having log(0) '''\n",
    "co_occurence_matrix = co_occurence_matrix + 1.0\n",
    "\n",
    "[num_classes, _] = co_occurence_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, num_classes, embed_size):\n",
    "\n",
    "        super(GloVe, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.in_embed.weight = xavier_normal(self.in_embed.weight)\n",
    "\n",
    "        self.in_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.in_bias.weight = xavier_normal(self.in_bias.weight)\n",
    "\n",
    "        self.out_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.out_embed.weight = xavier_normal(self.out_embed.weight)\n",
    "\n",
    "        self.out_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.out_bias.weight = xavier_normal(self.out_bias.weight)\n",
    "\n",
    "    def forward(self, word_u, word_v):\n",
    "\n",
    "        word_u_embed = self.in_embed(word_u)\n",
    "        word_u_bias = self.in_bias(word_u)\n",
    "        word_v_embed = self.out_embed(word_v)\n",
    "        word_v_bias = self.out_bias(word_v)\n",
    "        \n",
    "        return word_u_embed, word_v_embed, word_u_bias, word_v_bias\n",
    "\n",
    "    def embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy() + self.out_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GloVe (\n",
       "  (in_embed): Embedding(21536, 300)\n",
       "  (in_bias): Embedding(21536, 1)\n",
       "  (out_embed): Embedding(21536, 300)\n",
       "  (out_bias): Embedding(21536, 1)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = GloVe(num_classes, embed_size)\n",
    "if cuda_available:\n",
    "    glove = glove.cuda() # one gpu\n",
    "    # glove = torch.nn.DataParallel(glove, device_ids=[0,1,2,3]).cuda() # we set gpu in 1,2,3,4 slot. And it will be counted here with index 0 to 3.\n",
    "glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21536, 300])\n",
      "torch.Size([21536, 1])\n",
      "torch.Size([21536, 300])\n",
      "torch.Size([21536, 1])\n"
     ]
    }
   ],
   "source": [
    "for p in glove.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(glove.parameters(), l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_func(x):\n",
    "    return 1 if x > x_max else (x / x_max) ** alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size):\n",
    "    word_u = np.random.choice(np.arange(num_classes), size=batch_size, replace=False)\n",
    "    word_v = np.random.choice(np.arange(num_classes), size=batch_size, replace=False)\n",
    "\n",
    "    words_co_occurences = np.array([co_occurence_matrix[word_u[i], word_v[i]] for i in range(batch_size)])\n",
    "    words_weights = np.array([weight_func(var) for var in words_co_occurences])\n",
    "\n",
    "    words_co_occurences = Variable(torch.from_numpy(words_co_occurences).cuda()).float()\n",
    "    words_weights = Variable(torch.from_numpy(words_weights).cuda()).float()\n",
    "\n",
    "    word_u = Variable(torch.from_numpy(word_u).cuda())\n",
    "    word_v = Variable(torch.from_numpy(word_v).cuda())\n",
    "\n",
    "    return word_u, word_v, words_co_occurences, words_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 \t progress: 40.0 \t Loss: 0.000725\n",
      "Train Epoch: 0 \t progress: 60.0 \t Loss: 0.026821\n",
      "Train Epoch: 0 \t progress: 80.0 \t Loss: 0.003946\n",
      "Train Epoch: 0 \t progress: 100.0 \t Loss: 0.007126\n",
      "Train Epoch: 1 \t progress: 40.0 \t Loss: 0.010466\n",
      "Train Epoch: 1 \t progress: 60.0 \t Loss: 0.012776\n",
      "Train Epoch: 1 \t progress: 80.0 \t Loss: 0.017000\n",
      "Train Epoch: 1 \t progress: 100.0 \t Loss: 0.020834\n",
      "Train Epoch: 2 \t progress: 40.0 \t Loss: 0.018568\n",
      "Train Epoch: 2 \t progress: 60.0 \t Loss: 0.022954\n",
      "Train Epoch: 2 \t progress: 80.0 \t Loss: 0.024475\n",
      "Train Epoch: 2 \t progress: 100.0 \t Loss: 0.057465\n",
      "Train Epoch: 3 \t progress: 40.0 \t Loss: 0.036505\n",
      "Train Epoch: 3 \t progress: 60.0 \t Loss: 0.038287\n",
      "Train Epoch: 3 \t progress: 80.0 \t Loss: 0.037493\n",
      "Train Epoch: 3 \t progress: 100.0 \t Loss: 0.039739\n",
      "Train Epoch: 4 \t progress: 40.0 \t Loss: 0.053809\n",
      "Train Epoch: 4 \t progress: 60.0 \t Loss: 0.048651\n",
      "Train Epoch: 4 \t progress: 80.0 \t Loss: 0.042766\n",
      "Train Epoch: 4 \t progress: 100.0 \t Loss: 0.050464\n",
      "Train Epoch: 5 \t progress: 40.0 \t Loss: 0.050200\n",
      "Train Epoch: 5 \t progress: 60.0 \t Loss: 0.060064\n",
      "Train Epoch: 5 \t progress: 80.0 \t Loss: 0.066984\n",
      "Train Epoch: 5 \t progress: 100.0 \t Loss: 0.055151\n",
      "Train Epoch: 6 \t progress: 40.0 \t Loss: 0.076973\n",
      "Train Epoch: 6 \t progress: 60.0 \t Loss: 0.072859\n",
      "Train Epoch: 6 \t progress: 80.0 \t Loss: 0.060219\n",
      "Train Epoch: 6 \t progress: 100.0 \t Loss: 0.072908\n",
      "Train Epoch: 7 \t progress: 40.0 \t Loss: 0.051376\n",
      "Train Epoch: 7 \t progress: 60.0 \t Loss: 0.049656\n",
      "Train Epoch: 7 \t progress: 80.0 \t Loss: 0.085621\n",
      "Train Epoch: 7 \t progress: 100.0 \t Loss: 0.096466\n",
      "Train Epoch: 8 \t progress: 40.0 \t Loss: 0.063389\n",
      "Train Epoch: 8 \t progress: 60.0 \t Loss: 0.068502\n",
      "Train Epoch: 8 \t progress: 80.0 \t Loss: 0.076685\n",
      "Train Epoch: 8 \t progress: 100.0 \t Loss: 0.082013\n",
      "Train Epoch: 9 \t progress: 40.0 \t Loss: 0.091180\n",
      "Train Epoch: 9 \t progress: 60.0 \t Loss: 0.107778\n",
      "Train Epoch: 9 \t progress: 80.0 \t Loss: 0.111076\n",
      "Train Epoch: 9 \t progress: 100.0 \t Loss: 0.120881\n",
      "Train Epoch: 10 \t progress: 40.0 \t Loss: 0.113000\n",
      "Train Epoch: 10 \t progress: 60.0 \t Loss: 0.072024\n",
      "Train Epoch: 10 \t progress: 80.0 \t Loss: 0.110499\n",
      "Train Epoch: 10 \t progress: 100.0 \t Loss: 0.103014\n",
      "Train Epoch: 11 \t progress: 40.0 \t Loss: 0.090490\n",
      "Train Epoch: 11 \t progress: 60.0 \t Loss: 0.102151\n",
      "Train Epoch: 11 \t progress: 80.0 \t Loss: 0.104942\n",
      "Train Epoch: 11 \t progress: 100.0 \t Loss: 0.096695\n",
      "Train Epoch: 12 \t progress: 40.0 \t Loss: 0.110390\n",
      "Train Epoch: 12 \t progress: 60.0 \t Loss: 0.103473\n",
      "Train Epoch: 12 \t progress: 80.0 \t Loss: 0.109100\n",
      "Train Epoch: 12 \t progress: 100.0 \t Loss: 0.095787\n",
      "Train Epoch: 13 \t progress: 40.0 \t Loss: 0.094242\n",
      "Train Epoch: 13 \t progress: 60.0 \t Loss: 0.104879\n",
      "Train Epoch: 13 \t progress: 80.0 \t Loss: 0.116240\n",
      "Train Epoch: 13 \t progress: 100.0 \t Loss: 0.110832\n",
      "Train Epoch: 14 \t progress: 40.0 \t Loss: 0.121332\n",
      "Train Epoch: 14 \t progress: 60.0 \t Loss: 0.109493\n",
      "Train Epoch: 14 \t progress: 80.0 \t Loss: 0.114000\n",
      "Train Epoch: 14 \t progress: 100.0 \t Loss: 0.097866\n",
      "Train Epoch: 15 \t progress: 40.0 \t Loss: 0.135466\n",
      "Train Epoch: 15 \t progress: 60.0 \t Loss: 0.109849\n",
      "Train Epoch: 15 \t progress: 80.0 \t Loss: 0.138056\n",
      "Train Epoch: 15 \t progress: 100.0 \t Loss: 0.128807\n",
      "Train Epoch: 16 \t progress: 40.0 \t Loss: 0.109879\n",
      "Train Epoch: 16 \t progress: 60.0 \t Loss: 0.129644\n",
      "Train Epoch: 16 \t progress: 80.0 \t Loss: 0.130019\n",
      "Train Epoch: 16 \t progress: 100.0 \t Loss: 0.164720\n",
      "Train Epoch: 17 \t progress: 40.0 \t Loss: 0.113828\n",
      "Train Epoch: 17 \t progress: 60.0 \t Loss: 0.119603\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a5e571da1c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         ) * words_weights).sum()\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m100.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_cycle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mpercent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kdrl/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    num_batches = int(word_list_size/batch_size)\n",
    "    percent = 19.9\n",
    "    for batch_cycle in range(num_batches):\n",
    "        word_u, word_v, words_co_occurences, words_weights = next_batch(batch_size)\n",
    "        word_u_embed, word_v_embed, word_u_bias, word_v_bias = glove(word_u, word_v)\n",
    "        loss = (torch.pow(\n",
    "            ((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1) - torch.log(words_co_occurences), 2\n",
    "        ) * words_weights).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if 100. * batch_cycle / num_batches >= percent:\n",
    "            print('Train Epoch: {} \\t progress: {} \\t Loss: {:.6f}'.format(epoch, 100. * batch_cycle / num_batches, loss.data[0]))\n",
    "            percent += 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embeddings = glove.embeddings()  \n",
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3-4.4.0",
   "language": "python",
   "name": "anaconda3-4.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
