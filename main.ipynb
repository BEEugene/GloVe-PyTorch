{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib64:/usr/local/lib:/usr/lib64\r\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 21 14:03:14 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GT 610      Off  | 0000:01:00.0     N/A |                  N/A |\r\n",
      "| 40%   44C    P8    N/A /  N/A |      0MiB /   963MiB |     N/A      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN X (Pascal)    Off  | 0000:02:00.0     Off |                  N/A |\r\n",
      "| 26%   42C    P8    17W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN X (Pascal)    Off  | 0000:03:00.0     Off |                  N/A |\r\n",
      "| 27%   44C    P8    16W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  TITAN X (Pascal)    Off  | 0000:83:00.0     Off |                  N/A |\r\n",
      "| 28%   47C    P8    19W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  TITAN X (Pascal)    Off  | 0000:84:00.0     Off |                  N/A |\r\n",
      "| 29%   47C    P8    18W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0                  Not Supported                                         |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal, constant\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "if cuda_available:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "context_size = 5\n",
    "embed_size = 1000\n",
    "\n",
    "x_max = 10\n",
    "alpha = 0.75\n",
    "batch_size = 1000\n",
    "l_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.sys.ibm.pc.hardware']\n"
     ]
    }
   ],
   "source": [
    "categories = ['comp.sys.ibm.pc.hardware']\n",
    "raw_data = fetch_20newsgroups(subset = \"train\", categories = categories)\n",
    "print(list(raw_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_num 590\n"
     ]
    }
   ],
   "source": [
    "data_num = len(raw_data.data)\n",
    "print(\"data_num\",data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary and word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from badry cs ualberta ca ( badry jason theodore ) subject chaining ide drives summary trouble with master slave drives nntp posting host cab009 cs ualberta ca organization university of alberta , edmonton canada lines 16 hi i am trying to set up a conner 3184 and a quantum 80at drive i have the conner set to the master , and the quantum set to the slave ( does n't work the other way around ) i am able to access both drives if i boot from a floppy , but the drives will not boot themselves i am running msdos 6 , and have the conner partitioned as primary dos , and is formatted with system files i have tried all different types of setups , and even changed ide controller cards if i boot from a floppy , everything works great ( except the booting part ) ) the system does n't report an error message or anything , just hangs there does anyone have any suggestions , or has somebody else run into a similar problem ? i was thinking that i might have to update the bios on one of the drives ( is this possible ? ) any suggestions answers would be greatly appreciated please reply to jason badry badry cs ualberta ca\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    # Original : https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    # Tips for handling string in python : http://agiantmind.tistory.com/31\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "clean_str(raw_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents will be tokenized like ->  from badry cs ualberta ca ...\n"
     ]
    }
   ],
   "source": [
    "# Below line is needed to use nltk tokenizer\n",
    "# https://stackoverflow.com/questions/37101114/what-to-download-in-order-to-make-nltk-tokenize-word-tokenize-work\n",
    "# nltk.download(\"punkt\")\n",
    "print(\"All documents will be tokenized like -> \",*word_tokenize(clean_str(raw_data.data[0]))[:5],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list_size 145379\n",
      "vocab_size 11703\n"
     ]
    }
   ],
   "source": [
    "# This step might take time\n",
    "word_list = [word for data in raw_data.data for word in word_tokenize(clean_str(data))]\n",
    "vocab = np.unique(word_list)\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index: word for index, word in enumerate(vocab)}\n",
    "word_list_size = len(word_list)\n",
    "vocab_size = len(vocab)\n",
    "print(\"word_list_size\", word_list_size)\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct co-occurence matrix : This won't be generated if vocab_size is too big due to memory problem.\n",
    "co_occurence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "for i in range(word_list_size):\n",
    "    for j in range(1, context_size + 1):\n",
    "        index = word_to_index[word_list[i]]\n",
    "        if i-j > 0:\n",
    "            left_index = word_to_index[word_list[i-j]]\n",
    "            co_occurence_matrix[index, left_index] += 1.0/j\n",
    "        if i+j < word_list_size:\n",
    "            right_index = word_to_index[word_list[i+j]]\n",
    "            co_occurence_matrix[index, right_index] += 1.0/j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# co_oc Matrix is shifted in order to prevent having log(0)\n",
    "co_occurence_matrix = co_occurence_matrix + 1.0\n",
    "\n",
    "[num_classes, _] = co_occurence_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, num_classes, embed_size):\n",
    "\n",
    "        super(GloVe, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.in_embed.weight = xavier_normal(self.in_embed.weight)\n",
    "\n",
    "        self.in_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.in_bias.weight = xavier_normal(self.in_bias.weight)\n",
    "\n",
    "        self.out_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.out_embed.weight = xavier_normal(self.out_embed.weight)\n",
    "\n",
    "        self.out_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.out_bias.weight = xavier_normal(self.out_bias.weight)\n",
    "\n",
    "    def forward(self, word_u, word_v):\n",
    "\n",
    "        word_u_embed = self.in_embed(word_u)\n",
    "        word_u_bias = self.in_bias(word_u)\n",
    "        word_v_embed = self.out_embed(word_v)\n",
    "        word_v_bias = self.out_bias(word_v)\n",
    "        \n",
    "        return ((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1)\n",
    "    \n",
    "    def embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy() + self.out_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel (\n",
       "  (module): GloVe (\n",
       "    (in_embed): Embedding(11703, 1000)\n",
       "    (in_bias): Embedding(11703, 1)\n",
       "    (out_embed): Embedding(11703, 1000)\n",
       "    (out_bias): Embedding(11703, 1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = GloVe(num_classes, embed_size)\n",
    "if cuda_available:\n",
    "    # glove = glove.cuda() # one gpu\n",
    "    glove = torch.nn.DataParallel(glove, device_ids=[0,1,2,3]).cuda() # we set gpu in 1,2,3,4 slot. And it will be counted here with index 0 to 3.\n",
    "glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11703, 1000])\n",
      "torch.Size([11703, 1])\n",
      "torch.Size([11703, 1000])\n",
      "torch.Size([11703, 1])\n"
     ]
    }
   ],
   "source": [
    "for p in glove.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(glove.parameters(), l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_func(x):\n",
    "    return 1 if x > x_max else (x / x_max) ** alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/operation-between-tensor-and-variable/1286/4\n",
    "def next_batch(batch_size,word_u,word_v):\n",
    "\n",
    "    words_co_occurences = np.array([co_occurence_matrix[word_u[i], word_v[i]] for i in range(batch_size)])\n",
    "    words_weights = np.array([weight_func(var) for var in words_co_occurences])\n",
    "    \n",
    "    words_co_occurences = Variable(torch.from_numpy(words_co_occurences).cuda()).float()\n",
    "    words_weights = Variable(torch.from_numpy(words_weights).cuda()).float()\n",
    "\n",
    "    word_u = Variable(torch.from_numpy(word_u).cuda())\n",
    "    word_v = Variable(torch.from_numpy(word_v).cuda())\n",
    "\n",
    "    return word_u, word_v, words_co_occurences, words_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar(word_embeddings_array, word, result_num = 1):\n",
    "    data = []\n",
    "    num = word_embeddings_array.shape[0]\n",
    "    target_index = word_to_index[word]\n",
    "    for i in range(num):\n",
    "        if i != target_index:\n",
    "            data.append((index_to_word[i],cosine_similarity([word_embeddings_array[target_index]],[word_embeddings_array[i]])[0][0]))\n",
    "    data.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return data[:result_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \t Loss: 1.053742\n",
      "Similar words for 'system' :  [('massachusetts', 0.32696742), ('psu', 0.19651088), ('sharpness', 0.17754231), ('spl', 0.1699675), ('25130', 0.15909022)]\n",
      "Train Epoch: 2 \t Loss: 1.363482\n",
      "Similar words for 'system' :  [('cylinders', 0.42983997), ('xt', 0.4211376), ('extended', 0.28729102), ('sb', 0.196087), ('v6', 0.16135693)]\n",
      "Train Epoch: 3 \t Loss: 1.502517\n",
      "Similar words for 'system' :  [('tower', 0.45388857), ('midwest', 0.39337867), ('extended', 0.3279146), ('cylinders', 0.32209003), ('9', 0.31404656)]\n",
      "Train Epoch: 4 \t Loss: 1.308296\n",
      "Similar words for 'system' :  [('tower', 0.45125547), ('midwest', 0.33681771), ('9', 0.30914477), ('cylinders', 0.28699273), ('xt', 0.27460074)]\n",
      "Train Epoch: 5 \t Loss: 1.205730\n",
      "Similar words for 'system' :  [('tower', 0.43317813), ('9', 0.30084562), ('midwest', 0.29741156), ('xt', 0.26172471), ('extended', 0.25178793)]\n",
      "Train Epoch: 6 \t Loss: 1.200561\n",
      "Similar words for 'system' :  [('tower', 0.42451388), ('9', 0.25426871), ('midwest', 0.25400764), ('cylinders', 0.23751369), ('issued', 0.23677388)]\n",
      "Train Epoch: 7 \t Loss: 1.426552\n",
      "Similar words for 'system' :  [('tower', 0.39376646), ('subject', 0.38657236), ('9', 0.2446371), ('3rd', 0.2141878), ('win31', 0.20673274)]\n",
      "Train Epoch: 8 \t Loss: 1.476069\n",
      "Similar words for 'system' :  [('33', 0.52456135), ('tower', 0.34865183), ('subject', 0.30973223), ('9', 0.20383304), ('win31', 0.18694466)]\n",
      "Train Epoch: 9 \t Loss: 1.477755\n",
      "Similar words for 'system' :  [('33', 0.47550341), ('tower', 0.32398462), ('subject', 0.30113125), ('386sx', 0.27566302), ('9', 0.20047012)]\n",
      "Train Epoch: 10 \t Loss: 1.689611\n",
      "Similar words for 'system' :  [('boot', 0.38423431), ('33', 0.38123176), ('diskette', 0.32452846), ('tower', 0.24920483), ('subject', 0.22001722)]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # 각 에폭 당 한단어에 대해서 batch_size만큼의 짝을 찾아 batch_size번 업데이트함\n",
    "    losses = []\n",
    "    \n",
    "    random_word_u_indexes = np.random.permutation(num_classes)\n",
    "        \n",
    "    for word_u_position in range(0, num_classes, batch_size):\n",
    "        \n",
    "        word_u = random_word_u_indexes[word_u_position:(word_u_position + batch_size) if (word_u_position+batch_size) < num_classes else -1]\n",
    "        cycle_size = word_u.shape[0]\n",
    "        \n",
    "        random_word_v_indexes = np.random.permutation(num_classes)\n",
    "        \n",
    "        for word_v_position in range(0, num_classes, cycle_size):\n",
    "            \n",
    "            word_v = random_word_v_indexes[word_v_position:(word_v_position + cycle_size) if (word_v_position+cycle_size) < num_classes else -1]\n",
    "            \n",
    "            if cycle_size != word_v.shape[0]:\n",
    "                continue\n",
    "            \n",
    "            word_u_variable, word_v_variable, words_co_occurences, words_weights = next_batch(cycle_size, word_u, word_v)\n",
    "            forward_output = glove(word_u_variable, word_v_variable)\n",
    "            \n",
    "#             loss = sum([torch.mul((torch.dot(word_u_embed[i], word_v_embed[i]) +\n",
    "#                     word_u_bias[i] + word_v_bias[i] - np.log(words_co_occurences[i]))**2,\n",
    "#                     words_weights[i]) for i in range(cycle_size)])\n",
    "            \n",
    "#             print(type(((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1) - torch.log(words_co_occurences)))\n",
    "#             print(((((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1) - torch.log(words_co_occurences)) * words_weights).size())\n",
    "#             A = ((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1) - torch.log(words_co_occurences)\n",
    "#             print(forward_output.size())\n",
    "            loss = (torch.pow((forward_output - torch.log(words_co_occurences)), 2) * words_weights).sum()\n",
    "#             print(loss.size())\n",
    "            losses.append(loss.data[0])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    print('Train Epoch: {} \\t Loss: {:.6f}'.format(epoch + 1, np.mean(losses)))\n",
    "    print(\"Similar words for 'system' : \", most_similar(glove.module.embeddings(),\"system\",result_num=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3-4.4.0",
   "language": "python",
   "name": "anaconda3-4.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
