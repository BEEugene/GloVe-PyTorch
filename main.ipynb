{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries & GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib64:/usr/local/lib:/usr/lib64\r\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 21 17:55:36 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GT 610      Off  | 0000:01:00.0     N/A |                  N/A |\r\n",
      "| 40%   49C    P0    N/A /  N/A |      0MiB /   963MiB |     N/A      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN X (Pascal)    Off  | 0000:02:00.0     Off |                  N/A |\r\n",
      "| 25%   46C    P0    58W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN X (Pascal)    Off  | 0000:03:00.0     Off |                  N/A |\r\n",
      "| 27%   49C    P0    57W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  TITAN X (Pascal)    Off  | 0000:83:00.0     Off |                  N/A |\r\n",
      "| 28%   50C    P0    59W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  TITAN X (Pascal)    Off  | 0000:84:00.0     Off |                  N/A |\r\n",
      "|  0%   50C    P0    60W / 250W |      0MiB / 12189MiB |      1%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0                  Not Supported                                         |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal, constant\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "if cuda_available:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "context_size = 7\n",
    "embed_size = 300\n",
    "\n",
    "x_max = 50\n",
    "alpha = 0.75\n",
    "batch_size = 2000\n",
    "l_rate = 0.001\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves.urllib import request\n",
    "host = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
    "for name in [\"train\", \"valid\", \"test\"]:\n",
    "    request.urlretrieve(\n",
    "        \"{0}/ptb.{1}.txt\".format(host,name),\n",
    "        \"ptb.{0}.txt\".format(name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    # Tips for handling string in python : http://agiantmind.tistory.com/31\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kdrl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/19130512/stopword-removal-with-nltk\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "word_list = list()\n",
    "with open('ptb.train.txt') as f:\n",
    "    for line in f:\n",
    "        line = clean_str(line)\n",
    "        for word in line.split():\n",
    "            if word not in stop:\n",
    "                word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list_size 555226\n",
      "vocab_size 9519\n"
     ]
    }
   ],
   "source": [
    "# This step might take time\n",
    "vocab = np.unique(word_list)\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index: word for index, word in enumerate(vocab)}\n",
    "word_list_size = len(word_list)\n",
    "vocab_size = len(vocab)\n",
    "print(\"word_list_size\", word_list_size)\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct co-occurence matrix : This won't be generated if vocab_size is too big due to memory problem.\n",
    "co_occurence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "for i in range(word_list_size):\n",
    "    for j in range(1, context_size + 1):\n",
    "        index = word_to_index[word_list[i]]\n",
    "        if i-j > 0:\n",
    "            left_index = word_to_index[word_list[i-j]]\n",
    "            co_occurence_matrix[index, left_index] += 1.0/j\n",
    "            co_occurence_matrix[left_index, index] += 1.0/j\n",
    "        if i+j < word_list_size:\n",
    "            right_index = word_to_index[word_list[i+j]]\n",
    "            co_occurence_matrix[index, right_index] += 1.0/j\n",
    "            co_occurence_matrix[right_index, index] += 1.0/j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# co_oc Matrix is shifted in order to prevent having log(0)\n",
    "co_occurence_matrix = co_occurence_matrix + 1.0\n",
    "[num_classes, _] = co_occurence_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, num_classes, embed_size):\n",
    "\n",
    "        super(GloVe, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.in_embed.weight = xavier_normal(self.in_embed.weight)\n",
    "\n",
    "        self.in_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.in_bias.weight = xavier_normal(self.in_bias.weight)\n",
    "\n",
    "        self.out_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.out_embed.weight = xavier_normal(self.out_embed.weight)\n",
    "\n",
    "        self.out_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.out_bias.weight = xavier_normal(self.out_bias.weight)\n",
    "\n",
    "    def forward(self, word_u, word_v):\n",
    "\n",
    "        word_u_embed = self.in_embed(word_u)\n",
    "        word_u_bias = self.in_bias(word_u)\n",
    "        word_v_embed = self.out_embed(word_v)\n",
    "        word_v_bias = self.out_bias(word_v)\n",
    "        \n",
    "        return ((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1)\n",
    "    \n",
    "    def embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy() + self.out_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel (\n",
       "  (module): GloVe (\n",
       "    (in_embed): Embedding(9519, 300)\n",
       "    (in_bias): Embedding(9519, 1)\n",
       "    (out_embed): Embedding(9519, 300)\n",
       "    (out_bias): Embedding(9519, 1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = GloVe(num_classes, embed_size)\n",
    "if cuda_available:\n",
    "    # glove = glove.cuda() # one gpu\n",
    "    glove = torch.nn.DataParallel(glove, device_ids=[0,1,2,3]).cuda() # we set gpu in 1,2,3,4 slot. And it will be counted here with index 0 to 3.\n",
    "glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9519, 300])\n",
      "torch.Size([9519, 1])\n",
      "torch.Size([9519, 300])\n",
      "torch.Size([9519, 1])\n"
     ]
    }
   ],
   "source": [
    "for p in glove.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(glove.parameters(), l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_func(x):\n",
    "    return 1 if x > x_max else (x / x_max) ** alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/operation-between-tensor-and-variable/1286/4\n",
    "def next_batch(batch_size,word_u,word_v):\n",
    "\n",
    "    words_co_occurences = np.array([co_occurence_matrix[word_u[i], word_v[i]] for i in range(batch_size)])\n",
    "    words_weights = np.array([weight_func(var) for var in words_co_occurences])\n",
    "    \n",
    "    words_co_occurences = Variable(torch.from_numpy(words_co_occurences).cuda()).float()\n",
    "    words_weights = Variable(torch.from_numpy(words_weights).cuda()).float()\n",
    "\n",
    "    word_u = Variable(torch.from_numpy(word_u).cuda())\n",
    "    word_v = Variable(torch.from_numpy(word_v).cuda())\n",
    "\n",
    "    return word_u, word_v, words_co_occurences, words_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar(word_embeddings_array, word, result_num = 1):\n",
    "    data = []\n",
    "    num = word_embeddings_array.shape[0]\n",
    "    target_index = word_to_index[word]\n",
    "    for i in range(num):\n",
    "        if i != target_index:\n",
    "            data.append((index_to_word[i],cosine_similarity([word_embeddings_array[target_index]],[word_embeddings_array[i]])[0][0]))\n",
    "    data.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return data[:result_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \t Loss: 10.318272\n",
      "Similar words for 'programming' :  [('burned', 0.23667076), ('plane', 0.23507056), ('repairs', 0.19748446), ('hk', 0.19657624), ('plc', 0.19187272)]\n",
      "Train Epoch: 2 \t Loss: 15.368985\n",
      "Similar words for 'programming' :  [('plane', 0.23771545), ('burned', 0.23043594), ('repairs', 0.22823566), ('lsi', 0.19684684), ('limited', 0.19132639)]\n",
      "Train Epoch: 3 \t Loss: 11.808201\n",
      "Similar words for 'programming' :  [('bankamerica', 0.2707361), ('limited', 0.20636576), ('burned', 0.20176192), ('repairs', 0.2016439), ('maybe', 0.19468692)]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # 각 에폭 당 한단어에 대해서 batch_size만큼의 짝을 찾아 batch_size번 업데이트함\n",
    "    losses = []\n",
    "    \n",
    "    random_word_u_indexes = np.random.permutation(num_classes)\n",
    "        \n",
    "    for word_u_position in range(0, num_classes, batch_size):\n",
    "        \n",
    "        word_u = random_word_u_indexes[word_u_position:(word_u_position + batch_size) if (word_u_position+batch_size) < num_classes else -1]\n",
    "        cycle_size = word_u.shape[0]\n",
    "        \n",
    "        random_word_v_indexes = np.random.permutation(num_classes)\n",
    "        \n",
    "        for word_v_position in range(0, num_classes, cycle_size):\n",
    "            \n",
    "            word_v = random_word_v_indexes[word_v_position:(word_v_position + cycle_size) if (word_v_position+cycle_size) < num_classes else -1]\n",
    "            \n",
    "            if cycle_size != word_v.shape[0]:\n",
    "                continue\n",
    "            \n",
    "            word_u_variable, word_v_variable, words_co_occurences, words_weights = next_batch(cycle_size, word_u, word_v)\n",
    "            forward_output = glove(word_u_variable, word_v_variable)\n",
    "            \n",
    "#             loss = sum([torch.mul((torch.dot(word_u_embed[i], word_v_embed[i]) +\n",
    "#                     word_u_bias[i] + word_v_bias[i] - np.log(words_co_occurences[i]))**2,\n",
    "#                     words_weights[i]) for i in range(cycle_size)])\n",
    "            \n",
    "#             print(type(((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1) - torch.log(words_co_occurences)))\n",
    "#             print(((((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1) - torch.log(words_co_occurences)) * words_weights).size())\n",
    "#             A = ((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1) - torch.log(words_co_occurences)\n",
    "#             print(forward_output.size())\n",
    "            loss = (torch.pow((forward_output - torch.log(words_co_occurences)), 2) * words_weights).sum()\n",
    "#             print(loss.size())\n",
    "            losses.append(loss.data[0])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    print('Train Epoch: {} \\t Loss: {:.6f}'.format(epoch + 1, np.mean(losses)))\n",
    "    print(\"Similar words for 'programming' : \", most_similar(glove.module.embeddings(),\"programming\",result_num=5))\n",
    "    torch.save(glove.module.state_dict(), \"./glove.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bankamerica', 0.2707361),\n",
       " ('limited', 0.20636576),\n",
       " ('burned', 0.20176192),\n",
       " ('repairs', 0.2016439),\n",
       " ('maybe', 0.19468692)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove2 = GloVe(num_classes, embed_size)\n",
    "glove2.load_state_dict(torch.load(\"./glove.model\"))\n",
    "most_similar(glove2.embeddings(),\"programming\",result_num=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3-4.4.0",
   "language": "python",
   "name": "anaconda3-4.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
