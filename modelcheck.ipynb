{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameters...\n",
      "Define methods and classes...\n",
      "Prepare data...\n",
      "word_list_size 503550\n",
      "vocab_size 9462\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal, constant\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set parameters\n",
    "print(\"Set parameters...\")\n",
    "context_size = 10\n",
    "embed_size = 500\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "batch_size = 50\n",
    "l_rate = 0.001\n",
    "num_epochs = 30\n",
    "\n",
    "# define methods and classes\n",
    "print(\"Define methods and classes...\")\n",
    "def clean_str(string):\n",
    "    # Tips for handling string in python : http://agiantmind.tistory.com/31\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "def weight_func(x):\n",
    "    return 1 if x > x_max else (x / x_max) ** alpha\n",
    "\n",
    "def next_batch(batch_size,word_u,word_v):\n",
    "\n",
    "    words_co_occurences = np.array([co_occurence_matrix[word_u[i], word_v[i]] for i in range(batch_size)])\n",
    "    words_weights = np.array([weight_func(var) for var in words_co_occurences])\n",
    "    \n",
    "    words_co_occurences = Variable(torch.from_numpy(words_co_occurences).cuda()).float()\n",
    "    words_weights = Variable(torch.from_numpy(words_weights).cuda()).float()\n",
    "\n",
    "    word_u = Variable(torch.from_numpy(word_u).cuda())\n",
    "    word_v = Variable(torch.from_numpy(word_v).cuda())\n",
    "\n",
    "    return word_u, word_v, words_co_occurences, words_weights\n",
    "\n",
    "def most_similar(word_embeddings_array, word, result_num = 1):\n",
    "    data = []\n",
    "    num = word_embeddings_array.shape[0]\n",
    "    target_index = word_to_index[word]\n",
    "    for i in range(num):\n",
    "        if i != target_index:\n",
    "            data.append((index_to_word[i],cosine_similarity([word_embeddings_array[target_index]],[word_embeddings_array[i]])[0][0]))\n",
    "    data.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return data[:result_num]\n",
    "\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, num_classes, embed_size):\n",
    "\n",
    "        super(GloVe, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.in_embed.weight = xavier_normal(self.in_embed.weight)\n",
    "\n",
    "        self.in_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.in_bias.weight = xavier_normal(self.in_bias.weight)\n",
    "\n",
    "        self.out_embed = nn.Embedding(self.num_classes, self.embed_size)\n",
    "        self.out_embed.weight = xavier_normal(self.out_embed.weight)\n",
    "\n",
    "        self.out_bias = nn.Embedding(self.num_classes, 1)\n",
    "        self.out_bias.weight = xavier_normal(self.out_bias.weight)\n",
    "\n",
    "    def forward(self, word_u, word_v):\n",
    "\n",
    "        word_u_embed = self.in_embed(word_u)\n",
    "        word_u_bias = self.in_bias(word_u)\n",
    "        word_v_embed = self.out_embed(word_v)\n",
    "        word_v_bias = self.out_bias(word_v)\n",
    "        \n",
    "        return ((word_u_embed * word_v_embed).sum(1) + word_u_bias + word_v_bias).squeeze(1)\n",
    "    \n",
    "    def embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy() + self.out_embed.weight.data.cpu().numpy()\n",
    "    \n",
    "# prepare data\n",
    "print(\"Prepare data...\")\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "word_list = list()\n",
    "\n",
    "with open('ptb.train.txt') as f:\n",
    "    for line in f:\n",
    "        line = clean_str(line)\n",
    "        for word in line.split():\n",
    "            if word not in stop and len(word) > 1:\n",
    "                word_list.append(word)\n",
    "                \n",
    "vocab = np.unique(word_list)\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index: word for index, word in enumerate(vocab)}\n",
    "word_list_size = len(word_list)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"word_list_size\", word_list_size)\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = vocab_size\n",
    "glove = GloVe(num_classes, embed_size)\n",
    "glove.load_state_dict(torch.load(\"./glove.model\"))\n",
    "embedding_result = glove.embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parking\n",
      "[('con', 0.35182893), ('elliott', 0.34038278), ('psychological', 0.33453405), ('blues', 0.32255015), ('beautiful', 0.32028121)]\n"
     ]
    }
   ],
   "source": [
    "word = index_to_word[6000]\n",
    "print(word)\n",
    "print(most_similar(embedding_result,word,result_num=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mainframe', 0.27991056), ('compaq', 0.2648688), ('sunnyvale', 0.25959182), ('digital', 0.25702393), ('equipment', 0.24739763)]\n"
     ]
    }
   ],
   "source": [
    "print(most_similar(embedding_result,\"hardware\",result_num=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3-4.4.0",
   "language": "python",
   "name": "anaconda3-4.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
